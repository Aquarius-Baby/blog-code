---
date: 2020-02-01 16:15:03
categories:
   - 机器学习
tags:
   - 吴恩达视频笔记
mathjax: true
---
# 多变量线性回归
假设函数有多个变量：

$$
h_θ(x)=θ^Tx=θ_0+θ_1x+θ_2x+θ_3x
$$
<!--more-->

损失函数为：
$$
 J(θ)=\frac{1}{2m} \quad \sum_{i=1}^{m}{(h_θ(x^{(i)})-y^{(i)})^2}

 $$
 求偏导数：
  $$
θ_j:=θ_j-\alpha \frac{d}{dθ_j}J(θ)

$$
求取最小值的参数
$$
θ_0=θ_0-\alpha \frac{1}{m}\quad \sum_{i=1}^{m}{(h_θ(x^{(i)})-y^{(i)})}x_0^{(i)}
$$
 $$
θ_1=θ_1-\alpha \frac{1}{m}\quad \sum_{i=1}^{m}{(h_θ(x^{(i)})-y^{(i)})}x_1^{(i)}
$$
...

## 特征缩放
当特征之间的数量级别相差太多时，需要对特征的值进行处理，否则会影响梯度下降的速度。
$$
    x=\frac{x-x_{min}}{x_{max}-x_{min}}
$$
## 学习速率
如何选择：损失函数一直稳定的减少，某次迭代的降低速率在0.001以下。
- 过小：下降幅度过小。
- 过大：无法保证每一次迭代损失函数都在降低。



# 正规方程
比梯度下降方法更快的替代算法

假设训练样本的数据为X,对应的值为y，可利用数学求的其中的关系

$$
X=
\begin{bmatrix}
 1  &2014   &5  &2 &45\\ 
 1  &1416   &3  &2 &40\\ 
 1  &1534   &3  &2 &30\\
 1  &852    &2  &1 &36
\end{bmatrix}


y=
\begin{bmatrix}
 460 \\ 
 232 \\ 
 315 \\ 
 178 
\end{bmatrix}
$$
假设θ为
$$
θ=
\begin{bmatrix}
 θ_0 \\ 
 θ_1 \\ 
 θ_2 \\ 
 θ_3 \\
 θ_4
\end{bmatrix}
$$
可以求得：
 $$
θ=(X^TX)^{-1}X^Ty
$$
## 不可逆
如果求解时发现不可逆，可以选择去除一些特征，或者进行正则化
$$
(X^TX) is-non-invertible
$$

## 梯度下降与方程的区别
梯度下降：
- 需要选择学习效率
- 需要计算多次迭代
- 即使特征很多，也能很好的工作

方程求解：
- 无需选择学习效率
- 只需要进行一次数学计算
- 特征很多时，计算缓慢


# 分类问题--Logistic回归
## 分类

对回归计算进行处理
$$
 h_θ(x)= g(θ^{T}x)
$$

$$
 g(z)=\frac{1}{1+e^{-z}}
$$
得：

$$
y=\left\{\begin{matrix}0 &  h_\theta(x)< 0.5 \rightarrow θ^{T}x<0
\\ 
1 &  h_\theta(x) \geqslant  0.5\rightarrow θ^{T}x\geqslant0
\end{matrix}\right.
$$

## 代价函数
构造损失函数：
$$
Cost(h_θ(x),y)=\left\{\begin{matrix}-log(h_θ(x)) & if: y=1
\\ 
-log(1-h_θ(x)) & if: y=0
\end{matrix}\right.
$$
## 简化代价函数与梯度下降
$$
Cost(h_θ(x),y)=-ylog(h_θ(x))-(1-y)log(1-h_θ(x))
$$

$$
J(θ)=\frac{1}{m} \sum_{i=1}^{m}Cost(h_θ(x^{(i)}),y^{(i)})

=\frac{1}{m}[\sum_{i=1}^{m} y^{(i)}log(h_θ(x^{(i)}))+(1-y^{(i)})log(1-h_θ(x^{(i)}))]
$$
求使损失函数最小的θ。
类似梯度下降。

## 一对多
将3类样本进行分类。
设置3类分类器。
- 是A，是B or C
- 是B，是A or C
- 是C，是A or B
 
依次将样本代入分类器，概率最大的就是其所属分类


## 高级优化
梯度下降
共轭梯度法
BFGS
L-BFGS