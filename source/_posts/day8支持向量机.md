---
date: 2020-02-01 16:15:08
categories:
   - 机器学习
tags:
   - 吴恩达视频笔记
mathjax: true
---


# 优化目标
逻辑回归的损失函数：

<!--more-->

$$ 

J(θ)=-\frac{1}{m}[\sum_{i=1}^{m} y^{(i)}log(h_θ(x^{(i)}))+(1-y^{(i)})log(1-h_θ(x^{(i)}))] \\
+\frac{\lambda }{2m}\sum_{j=1}^{n}{\theta_j} ^2

$$

其中：
$$
h_θ(x^{(i)})=\frac{1}{1+e^{-\theta^T x^{(i)} }}
$$

画出坐标图，可以知道
$$
-log(h_θ(x^{(i)})) 
$$
随着x的增加，逐渐减少,收敛于0。

所以，y_i=0时，h(x)<<-1，
$$
-log(1-h_θ(x^{(i)})) \approx 0
$$

y_i=1时，h(x)>>1，
$$
-log(h_θ(x^{(i)})) \approx 0
$$



得到新的损失函数为：

$$
 min = \frac{1}{2} \sum_{i=1}^{n} \theta_j ^2
$$
同时，约束函数为
$$
\theta^Tx^{(i)} >>1  \; \; if \;  y^{(i)} = 1 

\theta^Tx^{(i)} << - 1  \; \; if \;  y^{(i)} = 0 
$$
# 大间隔
总结： 
1. SVM 要找到最中间的边界。 
2. 所以要找到最长的映射p。 
3. 进而可以找到所求参数θ的最小值。

# 核函数
我们设置f函数，衡量标记点和原先样本点的相似性



$$
    f_i = exp( -\frac{||x^{(i)}-l^{(i)}||^2}{2 \sigma ^2 })
$$
exp中的函数是高斯函数

在这种情况下，f的取值为：
x接近l时,f接近1。
x远离l时,f接近0。

δ越大，收敛越慢
δ越小，收敛越快


总结：
1.C 
- 大C:低偏差，高方差（对应低λ，overfitting）因为C约等于λ的倒数。 
- 小C:高偏差，低方差（对应高λ） 
2. δ 
- 大δ2：fi分布更平滑，高偏差，低方差 
- 小δ2：fi分布更集中，低偏差，高方差 （overfitting）


##### svm的使用
1. 特征维度n很大： 
使用逻辑回归和线性SVM. 
2. 特征维度n小，样本数量m中等： 
使用高斯核SVM。 
3. 特征维度n小，且样本数量m巨大： 
- 可以创建新的特征 
- 然后使用逻辑回归和无核SVM


https://blog.csdn.net/u012052268/article/details/78816779