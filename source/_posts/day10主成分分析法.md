---
date: 2020-02-01 16:15:10
categories:
   - 机器学习
tags:
   - 吴恩达视频笔记
toc: false
mathjax: true
---
PCA是一种无监督学习，是最常用的一种降维方法。

<!--more-->

# 应用场景<br/>
3D  --> 2D<br/>
2D  --> 1D

总结一下：<br/>
PCA的原理就是，为了将数据从n维降低到k维，需要找到k个向量，用于投影原始数据，使投影误差（投影距离）最小。

降维并不是直接去除其余的(n-k)维的特征，而是重新构造全新并且正交的k维特征。

PCA可以被定义为数据在低维线性空间上的正交投影，这个线性空间被称为主⼦空间（principal subspace），使得投影数据的⽅差被最⼤化（Hotelling, 1933），即==最大方差理论==。

等价地，它也可以被定义为使得平均投影代价最⼩的线性投影，即==最小误差理论==。平均投影代价是指数据点和它们的投影之间的平均平⽅距离。

还有另一个理论也可以解释PCA原理，即坐标轴相关度理论。这里简单探讨前两种，最后一种在讨论PCA意义时简单概述。

# 最大方差理论

在信号处理中认为信号具有较大的方差，噪声有较小的方差，信噪比就是信号与噪声的方差比，越大越好。因此我们认为，最好的k维特征是将n维样本点变换为k维后，每一维上的样本方差都尽可能的大。

首先，考虑在一维空间 (k=1) 上的投影。我们可以使用 n 维向量 u 定义这个空间的方 
向。为了方便(并且不失一般性)，我们假定选择一个单位向量，从而 
$$
u^Tu = 1
$$
 注意，我们只对u的方向感兴趣，而对 u 本身的大小不感兴趣。

假设原始数据集维X1，我们的目标是找到最佳的投影空间，

# 最小平方误差理论


# PCA运算步骤
1. 数据预处理。对X进行均值化，使其均值为0
2. 按行排列形成矩阵。
3. 计算协方差矩阵。cov_mat
3. 对协方差矩阵进行奇异值分解，得到U,S,V
4. 从U里面取前k个对应的特征向量按行组成矩阵P。
5. 通过计算Y=XP，得到降维后数据Y。

假设有n个样本，每个样本有m个特征
- x：n*m
- cov_mat：m*m
- U: m x m
- S: m x m
- V: m x m
- P：m x k
- Y: n x k

选取的k个特征向量对应k个特征根 λ，可以计算贡献度。
$$
\frac{\sum_{j=1}^{k} \lambda_j} {\sum_{i=1}^{n} \lambda_i  }
$$
表示其中可以隐藏的信息多少，一般选择贡献度85%以上。<br/>
PCA只是降低维度，簇并不一定与维度绑定，PCA的作用顶多就是去掉噪音，减少计算量，并不会剔除簇信息。    
    

# 数据复原
$$
Z = X * P

X = Z * P^T
$$
# PCA 与 线性回归的区别
- PCA属于无监督学习，其过程中没有label（y轴信息）；线性回归属于有监督学习，有label
- PCA的误差是用投影误差表示，而线性回归的误差是真实值（叉叉）与预测值（圈圈）之间的距离
- PCA的最终结果是投影点，而线性回归的最终结果是预测的直线



# 应用PCA的建议
一般应用
- 压缩数据，减少存储内存
- 适用于加速监督学习算法，一般保留99%的方差（一般可以减少1/5~1/10）
>   在训练集上使用，得到映射的函数将x-->z，然后将其用在交叉验证和验证集上
- 可视化

# 错误应用
利用PCA防止过拟合。


