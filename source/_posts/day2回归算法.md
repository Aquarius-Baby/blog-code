---
date: 2020-02-01 16:15:02
categories:
   - 机器学习
tags:
   - 吴恩达视频笔记
mathjax: true

---
# 回归算法
目标：寻找一个θ_0,θ_1使得预测值接近训练集中的样本
<!--more-->

$$
h_θ(x)=θ_0+θ_1x
$$

方法：预测值与真实值求差的平方。对于所有样本的该值进行求和，最后除以2m（1/2为简化计算），形成代价函数。

最小化该代价函数，得到对应的θ_0和θ_1。
 $$
 J(θ_0,θ_1)=\frac{1}{2n} \quad \sum_{i=1}^{n}{(h_θ(x^{(i)})-y^{(i)})^2}

 $$
代价函数有很多形式，平方误差是解决回归问题的最常用手段。

假设函数：抽象出来的数据模型，含有大量的参数变量。

代价函数：用来计算使假设函数和实际值差距最小的方法。进而确定假设函数中参数变量的值。
# 函数求解
求解假设函数的方式：
1. 先给两个参数一组值，
2. 求出对应的代价函数，画图
3. 寻找出图中代价函数的最小值

 用到了等高图的思想
 
# 梯度下降
找到合适的参数，使代价函数最小。
$$
\min_{θ_0,θ_1}J(θ_0,θ_1)=min \frac{1}{2n} \quad \sum_{i=1}^{n}{(h_θ(x^{(i)})-y^{(i)})^2}
$$

$$
θ_j:=θ_j-\alpha \frac{d}{dθ_j}J(θ_0,θ_1)
$$
求解得：
$$
θ_0:=θ_0-\alpha \frac{d}{dθ_0}(\frac{1}{2n} \quad \sum_{i=1}^{n}{(h_θ(x^{(i)})-y^{(i)})^2})
=θ_0-\alpha \frac{d}{dθ_0}(\frac{1}{2n} \quad \sum_{i=1}^{n}{(θ_0+θ_1x^{(i)}-y^{(i)})^2})

=θ_0-\alpha \frac{1}{n}(h_θ(x^{(i)})-y^{(i)})
$$

$$
θ_1:=θ_1-\alpha \frac{d}{dθ_1}(\frac{1}{2n} \quad \sum_{i=1}^{n}{(h_θ(x^{(i)})-y^{(i)})^2})
=θ_1-\alpha \frac{d}{dθ_1}(\frac{1}{2n} \quad \sum_{i=1}^{n}{(θ_0+θ_1x^{(i)}-y^{(i)})^2})

=θ_1-\alpha \frac{1}{n}(h_θ(x^{(i)})-y^{(i)})x^{(i)}
$$
# 学习效率
α表示学习率，梯度下降法是要同时更新θ_j的，梯度下降法是求局部最优解，受到初始值的影响
 
学习率α代表更新的幅度。
考虑偏导数项，考虑其正负，正或负都会将参数更新的方向引导向最低谷。

若学习率过小，更新效率会很低；
若学习率过大，或许会错过最低谷。

考虑梯度下降法已经将参数调整至一个局部低谷，若继续用梯度下降法，由于导数项为0，所以更新会停滞。

即使学习率不变，梯度下降法也可以找到局部低谷。当逼近局部最低点时，梯度下降法会自动减小学习率。

# batch梯度
batch梯度下降：使用全部样本数据的一种梯度下降方法。

求解线性回归，可以用梯度下降，也可以用方程组法，各有优点，前者更适合大数据集



# 问题
## 1.为什么神经网络使用梯度下降

因为可以得到局部最优解？

## 2.batch_size的不同有什么区别？1，部分，全部
Batch_size参数的作用:决定了下降的方向


batch_size为全数据集(Full Batch Learning)：
> - 好处：
> 1. 由全数据集确定的方向能够更好地代表样本总体，从而更准确地朝向极值所在的方向。
> 2. 由于不同权重的梯度值差别巨大，因此选择一个全局的学习率很困难。Full Batch Learning可以使用Rprop只基于梯度符号并且针对性单独更新各权值。
> - 坏处：
> 1. 随着数据集的海量增长和内存限制，一次性载入所有的数据进来变得越来越不可行。
> 2. 以Rprop的方式迭代，会由于各个Batch之间的采样差异性，各次梯度修正值相互抵消，无法修正。


batch_size=1:

> Batch_size=1，也就是每次只训练一个样本,这就是在线学习(Online Learning)。<br/>
线性神经元在均方误差代价函数的错误面是一个抛物面，横截面是椭圆。
对于多层神经元，非线性网络，在局部依然近似是抛物面。<br/>
使用在线学习，每次修正方向以各自样本的梯度方向修正，难以达到收敛。


需要选择合适的batch_size:

也就是批梯度下降法。因为如果数据集足够充分，那么用一半，甚至少得多的数据训练算出来的梯度与用全部数据训练出来的梯度几乎是一样的。

在合理范围内，增大Batch_size的好处：

1. 提高了内存利用率以及大矩阵乘法的并行化效率。
2. 减少了跑完一次epoch(全数据集）所需要的迭代次数，加快了对于相同数据量的处理速度。

盲目增大Batch_size的坏处：

1. 提高了内存利用率，但是内存容量可能不足。
2. 跑完一次epoch(全数据集)所需的迭代次数减少，要想达到相同的精度，其所花费的时间大大增加，从而对参数的修正也就显得更加缓慢。
3. Batch_size增大到一定程度，其确定的下降方向已经基本不再变化。


## ps:
>1. batch_size设的大一些，收敛得块，也就是需要训练的次数少，准确率上升的也很稳定，但是实际使用起来精度不高。
>2. batch_size设的小一些，收敛得慢，可能准确率来回震荡，因此需要把基础学习速率降低一些，但是实际使用起来精度较高。<br/><br/>
>一般尝试batch_size=64或者batch_size=1两种情况。


泰勒展开公式
通过迭代的方式寻找最优解
牛顿法与sgd的相同与不同