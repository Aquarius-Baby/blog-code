---
date: 2020-02-01 16:15:07
categories:
   - 机器学习
tags:
   - 吴恩达视频笔记
mathjax: true

---
# 误差分析
- 快速的选择一个模型实现，用交叉验证集去测试。
- 画出学习曲线，决定是需要更多的数据还是更多的特征，或者其他。
- 检查交叉验证集上的错误，尝试找到一个合适的点。
<!--more-->


例子：垃圾邮件的分类

例如，假设我们有500封电子邮件，我们的算法对其中的100封进行了错误分类。 我们可以手动分析100封电子邮件，并根据它们的电子邮件类型对其进行分类。 然后，我们可以尝试提供新的提示和功能，以帮助我们正确地分类这100封电子邮件。 因此，如果我们的大多数错误分类的电子邮件都是试图窃取密码的电子邮件，那么我们可以找到一些特定于这些电子邮件的功能并将它们添加到我们的模型中。 我们还可以看到如何根据其根对每个单词进行分类来改变我们的错误率：
$$ discount/discounting $$ 



将错误结果作为单个数值获取非常重要。 否则很难评估算法的性能。 例如，如果我们使用词干，那就是将不同形式的同一个词（失败/失败/失败）视为一个单词（失败）的过程，得到3％的错误率而不是5％，那么我们一定要把它添加到我们的模型中。

但是，如果我们试图区分大写和小写字母并结束得到3.2％的错误率而不是3％，那么我们应该避免使用这个新功能。因此，我们应该尝试新事物，获取错误率的数值，并根据我们的结果决定我们是否要保留新功能。

# 评测指标

场景：<br/>
假如某个班级有男生80人,女生20人,共计100人。目标是找出所有女生。<br/>
现在某人挑选出50个人,其中20人是女生,另外还错误的把30个男生也当作女生挑选出来了。
作为需要来评估(evaluation)下他的工作。

评估指标包括：准确率(Accuracy),精确率(Precision),召回率(Recall)和F1-Measure


## 准确率 accurancy
准确率(accuracy)的定义是: 对于给定的测试数据集，分类器正确分类的样本数与总样本数之比。也就是损失函数是0-1损失时测试数据集上的准确率[1].

前面的场景中，实际情况是那个班级有男的和女的两类，将班级中的人分为男女两类。accuracy需要得到的是分正确的人占总人数的比例。很容易，我们可以得到：其中70(20女+50男)人判定正确了,而总人数是100人，所以它的accuracy就是70%。

由准确率，我们的确可以在一些场合，从某种意义上得到一个分类器是否有效，但它并不总是能有效的评价一个分类器的工作。<br/>
举个例子,癌症的判断。癌症的出现可能行比较低（假设是0.02%），假如我将所有人都判断为没有癌症，那么正确率有99.98%，比几乎所有的算法的准确率都高，但这不是我们需要的算法。
这时候，就需要考虑使用precision,recall和f1-measure等评价指标了。

在说precision,recall和f1-measure之前,我们需要先需要定义TP,FN,FP,TN四种分类情况。
按照前面例子,我们需要从一个班级中的人中寻找所有女生,如果把这个任务当成一个分类器的话,那么女生就是我们需要的,而男生不是,所以我们称女生为"正类",而男生为"负类"



  &nbsp;|  相关(Relevant),正类 | 无关(NonRelevant),负类
---|---|---
被检索到(Retrieved) | true positives<br/>(TP 正类判定为正类)<br/> ==女判断为女==| false positives<br/>(FP 负类判定为正类) <br/>==男判断为女==
未被检索到(Not Retrieved) |false negatives<br/>(FN 正类判定为负类)<br/>==女判断为男==|true negatives<br/>(TN 负类判定为负类)<br/>==男判断为男==

所以在例子中
- TP=20
- FP=30
- FN=0
- TN=50


### 精确度 precision

$$
P = \frac{TP}{TP+FP}

$$
它计算的是所有"正确被检索的item(TP)"占所有"实际被检索到的(TP+FP)"的比例.

在例子中就是希望知道得到的所有人中,正确的人(也就是女生)占有的比例。所以其precision也就是40%(20女生/(20女生+30误判为女生的男生)).


## 召回率

$$ R = \frac{TP}{TP+FN} $$
它计算的是所有"正确被检索的item(TP)"占所有"应该检索到的item(TP+FN)"的比例。

在例子中就是希望知道此君得到的女生占本班中所有女生的比例,所以其recall也就是100%(20女生/(20女生+ 0 误判为男生的女生))

## F值
F1值就是精确值和召回率的调和均值,也就是

$$
\frac{2}{F_1} = \frac{1}{P} + \frac{1}{R} 
$$
得到

$$
F_1 = 2\frac{PR}{P+R}
$$


精确度就是找得对，召回率就是找得全。

精确度和召回率是互相影响的，理想情况下肯定是做到两者都高，但是一般情况下精确度高、召回率就低，召回率低、精确度高，当然如果两者都低，那是什么地方出问题了。一般情况，用不同的阀值，统计出一组不同阀值下的精确度和召回率。

如果是做搜索，那就是保证召回的情况下提升精确度；如果做疾病监测、反垃圾，则是保精确度的条件下，提升召回。

所以，在两者都要求高的情况下，可以用F1来衡量。


# ROC、AUC
  &nbsp;|  相关(Relevant),正类 | 无关(NonRelevant),负类
---|---|---
被检索到(Retrieved) | true positives<br/>(TP 正类判定为正类)<br/> ==女判断为女==| false positives<br/>(FP 负类判定为正类) <br/>==男判断为女==
未被检索到(Not Retrieved) |false negatives<br/>(FN 正类判定为负类)<br/>==女判断为男==|true negatives<br/>(TN 负类判定为负类)<br/>==男判断为男==

ROC(Receiver Operating Characteristic)关注两个指标
- True Positive Rate ( TPR )  = TP / [ TP + FN] ，TPR代表能将正例分对的概率
- False Positive Rate( FPR ) = FP / [ FP + TN] ，FPR代表将负例错分为正例的概率

在ROC 空间中，每个点的横坐标是FPR，纵坐标是TPR，这也就描绘了分类器在TP（真正的正例）和FP（错误的正例）间的trade-off。

ROC的主要分析工具是一个画在ROC空间的曲线——ROC curve。<br/>
我们知道，对于二值分类问题，实例的值往往是连续值，我们通过设定一个阈值，将实例分类到正类或者负类（比如大于阈值划分为正类）。<br/>
因此我们可以变化阈值，根据不同的阈值进行分类，根据分类结果计算得到ROC空间中相应的点，连接这些点就形成ROC curve。ROC curve经过（0,0）（1,1），实际上(0, 0)和(1, 1)连线形成的ROC curve实际上代表的是一个随机分类器。一般情况下，这个曲线都应该处于(0, 0)和(1, 1)连线的上方。
用ROC curve来表示分类器的performance很直观好用。
> 经过（0，0）表示将所有的实例全部分到负类。则，将正例分对的概率为0。负例分错的概率也为0。<br/>
> 经过（1，1）表示将所有的实例全部分到正类。则，将正例分对的概率为1。负例全部分错，概率也为1。

可是，人们总是希望能有一个数值来标志分类器的好坏。

于是Area Under roc Curve(AUC)就出现了。
顾名思义，AUC的值就是处于ROC curve下方的那部分面积的大小。<br/>
通常，AUC的值介于0.5到1.0之间，较大的AUC代表了较好的Performance。